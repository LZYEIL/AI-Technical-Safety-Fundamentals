{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "R7Ru-U7aMQVD",
        "lt8lH9bwLVmE"
      ],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üî• Week 0: Introduction to Machine Learning\n",
        "*Highly recommended for beginners in Machine Learning*"
      ],
      "metadata": {
        "id": "gdloNCG9ezSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Flashcards](https://quizlet.com/943164876/week-0-optional-flash-cards/?i=3mscu3&x=1jqt)"
      ],
      "metadata": {
        "id": "N2uewP7UnwQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Readings"
      ],
      "metadata": {
        "id": "tB6ihofsbji3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [A short introduction to Machine Learning - Richard Ngo](https://www.alignmentforum.org/posts/qE73pqxAZmeACsAdF/a-short-introduction-to-machine-learning) (10 mins)\n",
        "  * Survey the field of Machine Learning to understand its history and conceptually map out the territory\n",
        "*  3Blue1Brown - an explanation on deep learning\n",
        "  * [But what is a neural network? | Chapter 1, Deep learning](https://www.youtube.com/watch?v=aircAruvnKk&t=0s) (20 mins)\n",
        "    * Learn what a neural network is and how its architecture allows it to pick up on structure out of the data\n",
        "  * [Gradient descent, how neural networks learn | Chapter 2, Deep learning](https://youtu.be/IHZwWFHWa-w) (watch first 10 mins)\n",
        "    * Learn about loss functions and gradient descent\n",
        "*  LLMS and Scaling Laws\n",
        "  * [How ChatGPT works technically](https://www.youtube.com/watch?v=bSvTVREwSNw) (8 mins)\n",
        "    * Briefly survey how Large Language Models are made\n",
        "  * [Why and how of scaling large language models](https://www.youtube.com/watch?v=qscouq3lo0s) (10 mins)\n",
        "    * A brief introduction to scaling laws and their impact on foundation models"
      ],
      "metadata": {
        "id": "tli1vvk3brwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Readings"
      ],
      "metadata": {
        "id": "cflDoN2nblva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [But what is a GPT?  Visual intro to transformers | Chapter 5, Deep Learning](https://youtu.be/wjZofJX0v4M?si=VuUXz5fgbCeQmx4c) (27 mins)\n",
        "  * Take initial steps in understanding how Large Language Models work\n",
        "* [Visualizing Attention, a Transformer's Heart | Chapter 6, Deep Learning](https://youtu.be/eMlx5fFNoYc?si=ueB0hHuluYpBi8n9) (26 mins)\n",
        "  * Understand how attention, the main algorithmic advancement behind the success of Large Language Models, works\n",
        "* [Generalization, overfitting, and under-fitting in supervised learning](https://medium.com/mlearning-ai/generalization-overfitting-and-underfitting-in-supervised-learning-a21f02ebf3df) (10 mins)\n",
        "  * Learn about the basic fundamental definitions of generalization, overfitting, underfitting, and how they occur in machine learning models.\n",
        "* [Visualizing the Deep Learning revolution - Richard Ngo](https://medium.com/@richardcngo/visualizing-the-deep-learning-revolution-722098eb9c5) (20 mins)\n",
        "  * Survey the advances that have happened in AI across many different subfields"
      ],
      "metadata": {
        "id": "V3C5c2PPbqXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö†Ô∏è Week 1: Systems Thinking, AI Harms & Risks"
      ],
      "metadata": {
        "id": "7W0e6t_bOWQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Flashcards](https://quizlet.com/956691585/week-1-flash-cards/?i=3mscu3&x=1jqt)"
      ],
      "metadata": {
        "id": "6NmtBQd85YFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Session"
      ],
      "metadata": {
        "id": "7wqq65s_L0GZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- [Understanding AI Safety](https://understanding-ai-safety.org/)\n",
        "  - A broad overview of what AI Safety is and some practical concerns.\n",
        "- [The AI triad & national security strategy](https://cset.georgetown.edu/wp-content/uploads/CSET-AI-Triad-Report.pdf) (15 mins) [Executive Summary and Introduction, Optionally More]\n",
        "  * Learn about how and why the trajectory and speed of AI development depends on Compute, Data, and Algorithms.\n",
        "- [How Complex Systems Fails](https://how.complexsystems.fail/)"
      ],
      "metadata": {
        "id": "tfiaWKOoL5WX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In-Session\n"
      ],
      "metadata": {
        "id": "1sDpGvyaR8u5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Cohort First Meeting Intro](https://docs.google.com/document/d/15ByhulhQxVq2YATDQ_Z7FUuGXg97na6Zl4o6U_ZXkCI/edit?usp=sharing) (20-25 mins)\n",
        "  * Set the tone for the program.\n",
        "* [Unsolved problems in ML safety](https://montrealethics.ai/unsolved-problems-in-ml-safety/) (10 mins)\n",
        "  * Survey some of the main technical unsolved problems in ML Safety.\n",
        "* [AI Risks Outlined, by WAISI](https://docs.google.com/document/d/1qJGjCajCnPdQR29TwQ0QWtyBaV2-59lEsyRW1cmsGc8/edit?usp=sharing) (5 mins)\n",
        "  * Survey WAISI‚Äôs brief perspective outlining the risks of AI.\n",
        "\n",
        "\n",
        "## Activity (45 min)\n",
        "* [An overview of catastrophic AI risks](https://arxiv.org/pdf/2306.12001.pdf) (20 mins) [Read through parts at your interest]\n",
        "  * Learn about the different sources of large scale, catastrophic AI risk and the factors that cause them.\n",
        "* Partner up (either choose groups or be assigned).\n",
        "\n",
        "* Pick a risk from above, spend half an hour doing more research on that risk. Come up with arguments for why it is an important risk to consider, and if you can, technical cruxes which could reduce the magnitude/probability of this risk. Make sure to discuss with your partner during this time! We don't want the room to be totally silent!\n",
        "* After that time, everyone will come together to talk to the group about their risk and argue why it is important and concerning.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HZjJzonLSKFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Readings"
      ],
      "metadata": {
        "id": "xEU08rphSuU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [What failure looks like]() (25 mins)\n",
        "  * Two stories of how irresponsible AI development could lead to very poor world outcomes.\n",
        "* [Optional Paper](https://arxiv.org/abs/2109.13916)\n",
        "\n"
      ],
      "metadata": {
        "id": "xjGGMLjNS1OF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèÜ Week 2: Reward Specification\n"
      ],
      "metadata": {
        "id": "wofb4jNYSdGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Flashcards](https://quizlet.com/963040586/week-2-flash-cards/?i=3mscu3&x=1jqt)"
      ],
      "metadata": {
        "id": "Ke-RrkLI5joU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Session"
      ],
      "metadata": {
        "id": "A9gf6dMZbJhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Problem**\n",
        "\n",
        "*  üé¶ [The Hidden Complexity of Wishes](https://youtu.be/gpBqw2sTD08?si=I4eLT1crDfpbzny1) (12 mins)\n",
        "  * Why \"wishes,\" or rewards are hard to define, adding difficulty to determining reward functions in models."
      ],
      "metadata": {
        "id": "oW4i2N6fb3n2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In-Session\n"
      ],
      "metadata": {
        "id": "ilFIt0laTQev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   [Specification gaming: the flip side of AI ingenuity](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/) (10 mins)\n",
        "  * Seeing reward hacking in action!\n",
        "  * Incorrectly focused reward functions can cause an RL model to \"hack\" the rewards by greedily maximizing the reward instead of pursuing the long-term goal of the model.\n",
        "* [Reward Hacking ](https://www.youtube.com/watch?v=46nsTFfsBuc)\n",
        "(6 Mins)\n",
        "  * Explains the idea of reward hacking with simple examples\n",
        "* [Sycophancy to subterfuge: Investigating reward tampering in language models](https://www.anthropic.com/research/reward-tampering) (10 mins)\n",
        "  * This reading investigates how language models can generalize from sample specification gaming (like sycophancy) to more complex and concerning behaviors like reward tampering, even without explicit training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Potential Solutions**\n",
        "* üé¶ [Inverse Reinforcement Learning Example](https://www.youtube.com/watch?v=h7uGyBcIeII) (5 mins)\n",
        "  * [For more mathematical details!](https://thegradient.pub/learning-from-humans-what-is-inverse-reinforcement-learning/)\n",
        "\n",
        "\n",
        "<br>**The following readings introduce the idea of Reinforcement Learning from Human Feedback (RLHF). Make sure you understand the training setup used, as we'll refer back to it in future weeks.** <br>\n",
        "* [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf) (10 mins)\n",
        "* A cool example! [Summarizing books with Human Feedback](https://openai.com/blog/summarizing-books/) (5 mins)\n",
        "* [Weak to Strong Generalization](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf) (10 mins) [Introduction only]\n",
        "  * Understand another approach for using less powerful LLMs to align more powerful LLMs\n",
        "\n",
        "<br>\n",
        "\n",
        "**Discussion**\n",
        "* How would you define reward hacking in your own terms? Do you see elements of trying to \"game the system\" in human behavior as well?\n",
        "* How does reward hacking pose risks for AI safety?\n",
        "\n",
        "## Activity (45 mins)\n",
        "* [Examples of Specification Gaming](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml)\n",
        "\n",
        "* Split into pairs (either pick or assigned by facilitator).\n",
        "* Pick an example of Specification Gaming in the wild to read more about for half an hour. Make sure to discuss with your partner during this time! We don't want the room to be totally silent!\n",
        "* Then, come back together as a group to explain your example. Be sure to come up with an explanation for how this example occurred, and if you can think of any ways to prevent it from happening (e.g. by altering the reward), explain those as well."
      ],
      "metadata": {
        "id": "kBF-zCmnTfCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Readings\n"
      ],
      "metadata": {
        "id": "JPUJtlwHZK2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[The Value Learning Problem](https://intelligence.org/files/ValueLearningProblem.pdf) (Section 2 onwards)\n",
        "\n",
        "The paper covers the value learning problem in AI, where the model may misinterpret human values, leading to harmful outcomes. It also builds on using methods like inductive learning which act on human preferences. ( Kind of outdated but has important sections like Corrigibility, Ambiguity identification for preventing reward specification etc )\n",
        "\n",
        "[Zero-Shot Reward Specification via Grounded Natural Language](https://proceedings.mlr.press/v162/mahmoudieh22a/mahmoudieh22a.pdf) (25 Mins)\n",
        "(Till Section 5)\n",
        "\n",
        "Explores a new technique \"Contrastive Language-Image Pre-training\" which eliminates the use of state\n",
        "or image based demonstrations for specifying RL rewards. (Eliminates the use of retraining for new tasks in zero-shot-reward-models.)"
      ],
      "metadata": {
        "id": "L3ZR58s_aECc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§• Week 3: Generalization, Deception\n"
      ],
      "metadata": {
        "id": "0NXfC5CsUmIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Flashcards](https://quizlet.com/963038085/week-3-flash-cards/?i=3mscu3&x=1jqt)"
      ],
      "metadata": {
        "id": "nrkVfI7UrcrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Session\n",
        "\n",
        "- Review from Week 0: [Generalization, overfitting, and under-fitting in supervised learning](https://medium.com/mlearning-ai/generalization-overfitting-and-underfitting-in-supervised-learning-a21f02ebf3df) (5 mins)\n",
        "\n",
        "- [Goal misgeneralisation: Why correct specifications aren‚Äôt enough for correct goals](https://deepmindsafetyresearch.medium.com/goal-misgeneralisation-why-correct-specifications-arent-enough-for-correct-goals-cf96ebc60924) (20 mins)\n",
        "\n",
        "- Review: [Sycophancy to subterfuge: Investigating reward tampering in language models](https://www.anthropic.com/research/reward-tampering) (5 mins)\n",
        "  - This reading investigates how language models can generalize from sample specification gaming (like sycophancy) to more complex and concerning behaviors like reward tampering, even without explicit training.\n",
        "\n",
        "- [Generalization](https://youtu.be/lAKL7dCcBqk?feature=shared) (2 mins)\n",
        "\n",
        "## In-Session\n",
        "\n",
        "Review of Last Week‚Äôs Content (5-10 mins)\n",
        "- Goal Misgeneralization   \n",
        "    - [[Paper] Goal misgeneralisation] (15 mins) (https://arxiv.org/abs/2210.01790) - {Read only part 1 with one example in part 3 and full part 4}\n",
        "    - Argues that even an agent trained on the 'right' reward function might learn competent yet undesirable goals\n",
        "    - Paper with examples for goal misgeneralisation https://www.apartresearch.com/project/goal-misgeneralization (10 mins)\n",
        "- Instrumental Convergence\n",
        "  - [Why would AI want to do bad things?](https://youtu.be/ZeecOKBus3Q?feature=shared0) (15 mins)\n",
        "    - This resource discusses the idea of ‚Äòinstrumental goals‚Äô and how ML systems could maximize reward in unexpected ways: seeking power.\n",
        "- Is Deceptive Alignment Possible?\n",
        "  - [Deceptive AI does not equal Deceptively Aligned AI](https://www.alignmentforum.org/posts/a392MCzsGXAZP5KaS/deceptive-ai-deceptively-aligned-ai) (5 mins)  \n",
        "    - Understand the distinction between deceptive AI and deceptively aligned AI and think about the spectrum between the two\n",
        "- [AI Sandbagging: Language Models can Strategically Underperform on Evaluations](https://arxiv.org/abs/2406.07358) (15 mins)\n",
        "  - This reading demonstrates that current models can selectively underperform on dangerous capability evaluations while maintaining general performance, target specific accuracy levels, and even emulate less capable models\n",
        "- [Anthropic‚Äôs Sleeper Agents Paper Summary](https://jrodthoughts.medium.com/can-llms-be-deceptive-inside-anthropics-sleeper-agents-research-b88e4c71f701) (15 mins)    \n",
        "  - Explore Anthropic‚Äôs landmark paper exploring the scenarios where AI models might be deceptive and how our best methods currently fail to prevent this\n",
        "- [Prover Verifier Games Improve Legibility](https://openai.com/index/prover-verifier-games-improve-legibility/) (15 mins)\n",
        "  - Understand OpenAI‚Äôs approach to outputting model explanations that are legible and auditable\n",
        "- Out of Distribution Detection\n",
        "  - [Out-of-Distribution detection: A survey](https://arxiv.org/pdf/2110.11334.pdf) (15 mins) [Section 1, 2.4-2.5]\n",
        "\n",
        "## Activity (15 mins)\n",
        "* Phew, that was a lot of reading!\n",
        "* Let's come together to make a mindmap on the whiteboard/chalkboard of what we learned today. How does Instrumental Convergence fit in with Deceptive Alignment? How does OOD try to reduce some of these risks?\n",
        "* Draw connections between different topics discussed today.\n",
        "\n",
        "## Optional Readings\n",
        "- [[Paper] Goal misgeneralisation](https://arxiv.org/abs/2210.01790) - Remaining parts"
      ],
      "metadata": {
        "id": "sFP2rO5vUxvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úç Week 4: Prompting, Evaluations, and Adversarial Attacks\n",
        "\n"
      ],
      "metadata": {
        "id": "jHei5UUn6GCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Flashcards](https://quizlet.com/963046205/week-4-flash-cards/?i=3mscu3&x=1jqt)"
      ],
      "metadata": {
        "id": "Mdbbgy3ts1Wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Session"
      ],
      "metadata": {
        "id": "we2FYoXvKM8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Flashcards\n",
        "*   Intro to prompt engineering (videos [8 min](https://youtu.be/RztXj3o-e84?feature=shared) or [3 min](https://youtu.be/sW5xoicq5TY?feature=shared))\n",
        "*   [Universal and Transferable LLM Attacks](https://youtu.be/l080-kclS-A?feature=shared) (video 7 min)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0TPvsW_IUaRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In-Session"
      ],
      "metadata": {
        "id": "sA-7OtKu6oBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Review/feedback form time (5-10 min)\n",
        "\n"
      ],
      "metadata": {
        "id": "c_h7wPR85t89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompting** (30 minutes)\n",
        "* [What should an AI‚Äôs personality be? Anthropic](https://www.anthropic.com/research/claude-character) (10 minutes)\n",
        " * Read Anthropic‚Äôs thought on how they design the character and personality of their LLMs\n",
        "*   [LLM Prompt Engineering](https://medium.com/thedeephub/llm-prompt-engineering-for-beginners-what-it-is-and-how-to-get-started-0c1b483d5d4f) (Blogpost 10 minutes)\n",
        " * Read about the basics of prompting and the different prompt techniques such as few-shot and chain of thought prompting\n",
        "* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903) [Section 2-3.2] (10 minutes)\n",
        "  * In-depth look into chain of though prompting\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F0xyb3HEKedQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity (30 mins)\n",
        "*   [Take time to prompt LLMs on your own](https://lmarena.ai/)\n",
        " * Prompt any LLM of your choice using the different strategies you have learned and compare the responses (you can use the linked website to compare random LLMs)\n",
        " * Try asking for Chain of Thought. Give multiple examples before asking a language model to do something. Try to use simple jailbreaking techniques to get it to do something it shouldn't.\n",
        " * Also, try the [RedTeam Arena](https://redarena.ai/). Warning: This will feature offensive, potentially harmful language, so doing this is strictly optional.\n",
        " * Share cool examples of things you find!\n"
      ],
      "metadata": {
        "id": "KthT41RbGjyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adversarial Attacks** (30 minutes)"
      ],
      "metadata": {
        "id": "bXLdaHMNSsE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* [Universal and Transferable attacks against LLMs](https://arxiv.org/pdf/2307.15043) (figures 1 and 5, 10 minutes)\n",
        "*   [Example of adversarial attack](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm) (10 min)\n",
        "*   [Example of adversarial training](https://adversarial-ml-tutorial.org/adversarial_training/) (10 min skim)\n",
        "\n"
      ],
      "metadata": {
        "id": "GsSvn1hKS1xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Evaluating LLMs** (~25 minutes)"
      ],
      "metadata": {
        "id": "SVH88XYxMugF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* [Discovering Language Model Behaviors with Model-Written Evaluations](https://www.lesswrong.com/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written) (15 minutes)\n",
        "  * Using Language Models to generate datasets to evaluate other language models with. Note that Self-Evaluations likely are biased\n",
        "*   [Evaluating Large Language Models](https://cset.georgetown.edu/article/evaluating-large-language-models/) (10 minutes)\n",
        "  *   Understand the best practices for designing benchmarks to evaluate AI models with\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6hNfh6ViMxnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional"
      ],
      "metadata": {
        "id": "Z1gpwT6r5bNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   [Humanity's Last Exam](https://scale.com/blog/humanitys-last-exam)\n",
        "  * Read about CAIS's ambitious plans to create the benchmark to end all benchmarks.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t_i88XQM5eb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîé Week 5: Interpretability\n",
        "*How can we understand what's happening inside a model?*"
      ],
      "metadata": {
        "id": "vUqAO3G25SGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Session"
      ],
      "metadata": {
        "id": "TsPTBnG0I5tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Flashcards\n",
        "* [Intro to Probing](https://youtu.be/HJn-OTNLnoE?feature=shared) (11 mins)\n",
        "  * Learn the basics of probing techniques to try to understand what features a neural network has extracted at different points\n",
        "* [Intro to Mechanistic Interpretability](https://aisafetyfundamentals.com/blog/introduction-to-mechanistic-interpretability/) (10 mins)\n",
        "  * High level overview of mechanistic interpretability, a way of understanding human-interpretable algorithms learned by neural networks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PnBRqFKGhJ2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In-Session"
      ],
      "metadata": {
        "id": "NyVigigPLzUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Feature Visualization](https://distill.pub/2017/feature-visualization/) (25 minutes)\n",
        "  * Learn about *features* in Computer Vision models, where there are often interpretable neurons\n",
        "* [Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/) (25 minutes)\n",
        "  * Learn about *circuits*, a fundamental unit consisting of connections between different features, which make human-interpretable algorithms which can explain some of neural network performance (maybe)\n",
        "* [Attention Explained by 3b1b](https://www.youtube.com/watch?v=eMlx5fFNoYc) (25 minutes)\n",
        "  * Understand the *attention* mechanism of transformers, the core operation which sets them apart. Then, we will visualize attention in Vision Transformers, which can help us get some sense of what they are doing (though this is very limited)\n",
        "\n",
        "\n",
        "## Activity (25 mins)\n",
        "* [NeuroScope](https://neuroscope.io/)\n",
        "\n",
        "\n",
        "Find some cool features/neurons, and share what you found with your neighbors. This shouldn't be a silent time!\n"
      ],
      "metadata": {
        "id": "5GKPskQ6nE5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Activities"
      ],
      "metadata": {
        "id": "h05HOlMHL1hT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Neel Nanda's List of Mechanistic Interpretability Papers](https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite)\n",
        "* [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) (35 minutes, read through \"Demonstrating Superposition\")\n",
        "  * Learn about *superposition*, a phenomenon which makes the simple neuron visualization techniques pretty much useless.\n",
        "* [Sparse Autoencoders (Scaling Monosemanticity)](https://transformer-circuits.pub/2024/scaling-monosemanticity/) (35 minutes)\n",
        "  * Learn about *sparse autoencoders*, a method which seeks to extract features from superposition.\n",
        "* INTERACTIVE [Scaling Monosemanticity Features](https://transformer-circuits.pub/2024/scaling-monosemanticity/umap.html?targetId=1m_570621)\n",
        "\n"
      ],
      "metadata": {
        "id": "svvwgnjIhLb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìï Week 6: AI Control Research\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SF9frJgYC4N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-Session\n"
      ],
      "metadata": {
        "id": "RlG6KeSjsxta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Machine Unlearning in Computer Vision](https://www.youtube.com/watch?v=0T9nUym4Hv4&t=1094s) (15:42 - 23:09)\n",
        "\n"
      ],
      "metadata": {
        "id": "jDeyc5g2ZAAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In-Session\n"
      ],
      "metadata": {
        "id": "HqNpAOqVs7c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [The Case for Ensuring That Powerful AIs Are Controlled](https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled)\n",
        "  * Argue for and methods of controlling powerful AI agents, and learning how to still have control despite potential misalignment.\n",
        "\n",
        "## More Discussion\n",
        "* Limitations of Interpretability and Open Questions\n",
        "  * Though techniques like SAEs are promising, they are still limited, and interpretability is far from \"solved\". We still don't understand that much of how models work.\n",
        "\n",
        "* [Systemic Safety](https://www.aisafetybook.com/textbook/systemic-safety#:~:text=For%20example%2C%20AI%20could%20be,to%20flag%20abnormal%20network%20traffic) (5 min)\n",
        "  * Learn about how AI and AI Safety interact with other systems."
      ],
      "metadata": {
        "id": "le9dgI4ns_5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optional\n"
      ],
      "metadata": {
        "id": "6iL8uzD-tCTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Characterizing Machine Unlearning through Definitions & Implementations](https://www.youtube.com/watch?v=i6Hxuvy8w5E)\n",
        "* [Intro to Machine Unlearning](https://ai.stanford.edu/~kzliu/blog/unlearning) (20 min)\n",
        "  * Learn about Machine Unlearning, a collection of techniques to try to remove harmful knowledge and capabilities from models.\n"
      ],
      "metadata": {
        "id": "XT_ni4rftEq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üèõ Week 7: Governance/Policy"
      ],
      "metadata": {
        "id": "2iUKhTNtZtZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Session"
      ],
      "metadata": {
        "id": "kVpQo0M-hMSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* üé¶ [AI Strategy, Policy, and Governance | Allan Dafoe](https://youtu.be/2IpJ8TIKKtI?si=XOP4WCcYOU5nAN96) (15 mins)\n",
        "  * Just watching the middle sections!\n",
        "* [Why and How Governments Should Monitor AI Development](https://arxiv.org/abs/2108.12427) (5 mins) - *Executive Summary*\n",
        "  * Introduces why AI Governance is necessary, why it may be difficult, and how we should go about it."
      ],
      "metadata": {
        "id": "hNlluVZKhP7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In-Session"
      ],
      "metadata": {
        "id": "vF6--h3LZ5FF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Anonymous Feedback Form](https://forms.gle/wH9iT1YjmJYFBXcm6) (15 mins)\n",
        "\n",
        "* [Global AI Talent Tracker](https://macropolo.org/digital-projects/the-global-ai-talent-tracker/) üåü (5 mins)\n",
        "  * Where AI talent is around the world!\n",
        "\n",
        "* **We must improve our governance.**\n",
        "  * [AI Governance needs technical work](https://forum.effectivealtruism.org/posts/BJtekdKrAufyKhBGw/ai-governance-needs-technical-work) (10 mins)\n",
        "  * Outlines why governance needs people with more than just humanities degrees; there exists a need for technical competence in governance.\n",
        "\n",
        "* [Open-Sourcing Highly Capable Foundation Models](https://www.governance.ai/research-paper/open-sourcing-highly-capable-foundation-models) (45-60 mins)\n",
        "  * Outlines the advantages and disadvantages of the most controversial subject in the field of AI right now: open sourcing powerful models.\n",
        "\n",
        "## Activity (45 minutes)\n",
        "Read the above paper, then, the facilitator will split cohort into two groups, one for open source, one for closed source. Then, take 20 minutes with your group to prepare your argument. You may not (in fact, probably won't) agree with everything that you will argue, but it's an important skill to be able to hold viewpoints which you may be opposed to in your head, in order to truly understand the viewpoint.\n",
        "<br><br>\n",
        "**Discussion** <br>\n",
        "* What are your concerns and predictions about how AI might look in society 2 years from now? 10 years? more?\n",
        "* What are some policy changes from these that you see as effective or ineffective?\n",
        "* How do we keep both government bodies and the general societal landscape well-informed about AI?\n",
        "  * Do you think AI currently is correctly interpreted by society? Explain yes/no.\n",
        "\n",
        "**End by talking through further involvement and career exploration**"
      ],
      "metadata": {
        "id": "TybhPPiIaILh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Readings"
      ],
      "metadata": {
        "id": "FJVksGs-Z7K_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Model Evaluations for Extreme Risks](https://arxiv.org/pdf/2305.15324.pdf) (30 mins)\n",
        "* [Evaluating Language-Model Agents on Realistic Autonomous Tasks](https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf) (30 mins)"
      ],
      "metadata": {
        "id": "h9_2lYuBaI_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Further Involvement and Career Explorations\n",
        "*What's next? How can you stay involved?*"
      ],
      "metadata": {
        "id": "Ls0Y3j7o6FVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Semester Reflection\n",
        "* Culture Reflection: [WAISI Culture Statement](https://docs.google.com/document/d/1KUVD7c-ZM0uijD0RvYYk6yBl0Oe09AAgIvYyoVUj4h8/edit?usp=sharing)  (5 mins)\n",
        "* Anonymous Feedback Forms (to be created) (10-15 mins)\n",
        "\n",
        "#### Further Involvement Discussion\n",
        "* [Safety Scholars](https://docs.google.com/document/d/1npT_7Svl_Gp10-BV8yusx6A90OokOflWFgCagKKU0G8/edit?usp=sharing) (10 mins)\n",
        "  * Our core membership, with access to our Research Network, ARENA-based upskilling, and weekly meetings to read papers, write code, and meet with speakers.\n",
        "* [Leadership](https://docs.google.com/document/d/101uApDOiusyDpgn7FFo-ikANA6GRpWHPESnQwMrMajE/edit?usp=sharing) (10 mins)\n",
        "  * Particularly enjoyed our programs? Think you'd like to lead an intro cohort, run events, or help in an executive capacity? WAISI leadership gets automatic admittance into Safety Scholars, preferential access to our resources, and valuable experience working with highly motivated leaders.\n",
        "\n",
        "We recommend completing the wrap-up outline to help prepare for Safety Scholars and Leadership applications. See the optional activities section for more details. **Applications will be open ________________ and close ________________**\n",
        "\n",
        "#### Career Advice\n",
        "* **Build Transferable Skills**: Technical AI Safety is a worthwhile field where skills are widely transferable. The expertise you gain here can be leveraged across multiple domains.\n",
        "* **Build Career Capital**: We encourage you to explore options that build important skills and demonstrate your competence for your resume even if a job isn't directly in Technical AI Safety. For example: Software engineering at a bank, TAs for relevant courses, etc.\n",
        "* **Explore Multiple Pathways**: Consider graduate school, software engineering, and machine learning engineering roles. These experiences will be invaluable if you decide to transition into Technical AI Safety later in your career.\n",
        "* **Diversify Your Skillset**: Acquire expertise in various fields to broaden your career options. Don't limit your goal to becoming an AI Alignment Engineer right after college."
      ],
      "metadata": {
        "id": "R7Ru-U7aMQVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Activities\n"
      ],
      "metadata": {
        "id": "lt8lH9bwLVmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Wrap-up Outline](https://docs.google.com/document/d/1dYY8SGKjhPrLO3ogkv0ItRYieBzwfbHd0AapqwDiuUw/edit?usp=sharing)\n",
        "  * Working through this will significantly help you in the technical knowledge assessment portions of our Safety Scholars applications.\n",
        "  * We also will provide you with the option to attach your completed document to your application, which very much will strengthen how we view you.\n",
        "  * DM us with any questions you have or feedback you‚Äôd like on your wrap-up! We are always excited to go back and forth discussing the meta of the field.\n",
        "\n",
        "* [Careers in Alignment](https://docs.google.com/document/d/1iFszDulgpu1aZcq_aYFG7Nmcr5zgOhaeSwavOMk1akw/edit?usp=sharing)\n",
        "  * Extra resources to think about a career in Technical AI Safety.\n"
      ],
      "metadata": {
        "id": "geNgROA_Sxfn"
      }
    }
  ]
}